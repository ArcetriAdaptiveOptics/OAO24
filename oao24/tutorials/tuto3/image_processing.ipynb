{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. Image Processing\n","## 1.1 Introduction\n","\n","The primary goal of adaptive optics (AO) is to correct for atmospheric turbulence in real time, allowing astronomers to obtain images that approach the diffraction limit of the telescope. While AO can greatly improve image sharpness, several **post-processing** steps are still required to optimise the scientific return. \n","\n","Typically in observing campaigns, before doing science on the collected image, the series of raw data are reduced using *auxiliary* images. These are calibration images that are needed to calibrate cientific image (with photometry o spectroscopy) to correct for instrumental effects.\n","\n","These *auxiliary* images are mainly of 3 types:\n","\n","- **Bias images**: these are taken with zero exposure time and a closed shutter (no incoming photons). They measure the electronic offset introduced by the CCD readout process.\n","\n","- **Flat field images**: these are obtained by imaging a uniformly illuminated source and allow us to measure the spatial variation in the response of the optical system and detector.\n","\n","- **Dark images**: these are acquired with the same exposure time of the science image but with the shutter closed and used to remove dark current from the detector, which is a contributor to the thermal noise signal that needs to be considered, especially when our target is a faint object. Typically, these frames are neglected in the image reduction routine, as detectors are cooled to cryogenic temperatures.\n","\n","So as shown in the following scheme, the processing of the collected raw images $S_{raw}$ can be expressed by the following relation:\n","\n","$$\n"," S_{red} = \\frac{S_{raw} - B}{F - B} \n","$$ \n","\n","where $S_{red}$ is the science image after the reduction of the bias, $B$, and the normalization to the flat field, $F$."]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"correction of science image.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["\n","However, in the context of our current dataset, we will focus specifically on **background subtraction** from the collected images, without the use of *auxiliary* calibration images. This approach allows us to focus on removing unwanted background noise to enhance the signal of astronomical objects without applying full instrumental calibration.\n","\n","If you want learn more about image processing techinques in see [Chap.6 Observing and calibration strategies (J. Heidt, 2022)](https://ui.adsabs.harvard.edu/abs/2022ASSL..467.....H/abstract).\n","\n","So at the end of this section, we will have a cleaned up closed loop data cube image from detector noise and sky background."]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Background Subtraction\n","\n","The **background noise** comes from various sources such as :\n","\n","1. sky brightness, like sky-background or other sources in the field close to the scientific target;\n","\n","2. detector noise and artifacts, some pixel of the camera may have larger readout noise than others \n","\n","3. unwanted signals from the telescope (tracking drifts, ghosts from anti-reflective coatings on the optics, static aberrations, among others ...)."]},{"cell_type":"markdown","metadata":{},"source":["In the following tutorial, we propose a simple background subtraction of open and closed loop data collected on *Beta Pegasi*, with PAPYRUS NIR  imager camera [C-RED3](https://andor.oxinst.com/products/c-red-series/c-red-3) camera.\n","\n","Let's start by loading the backup data and importing the needed modules."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["!pip uninstall -y oao24\n","!pip install git+https://github.com/ArcetriAdaptiveOptics/OAO24.git"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from oao24 import package_data"]},{"cell_type":"markdown","metadata":{},"source":["The `InfraredExampleData` class defined in 'package_data.py' is used to load the backup files, where:\n","\n","- `get_open_loop_data_cube()`: Loads and returns the open loop data cube from the file `\"ID_105.npy\"`.\n","\n","- `get_camera_dark_data()`: Loads and returns a background region from the file `\"ID_110.npy\"`.\n","\n","- `get_close_loop_data_cube()`: Loads and returns the closed loop data cube from the file `\"ID_109_red_16.npy\"`."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class InfraredExampleData():\n","    \n","    @staticmethod\n","    def get_open_loop_data_cube():\n","        image =  np.load(package_data.tuto3_folder() / \"ID_105.npy\")\n","        return np.atleast_3d(image)\n","    \n","    @staticmethod\n","    def get_camera_dark_data():\n","        return np.load(package_data.tuto3_folder() / \"ID_110.npy\")\n","    \n","    @staticmethod\n","    def get_close_loop_data_cube():\n","        return np.load(package_data.tuto3_folder() / \"ID_109_red_16.npy\")"]},{"cell_type":"markdown","metadata":{},"source":["We can define useful methods that implements raw images cleanup, implements sky subtractions and  perform cube stacking, to get our final processed image (*master*).\n","\n","We can implement functions like the ones definded in 'image_pocesing.py'.\n","\n","To inspect the **sky-backgroud** across on our image, we can define a function like:\n","\n"," - `print_roi_mean_values()`:\n","  selects regions of interest (ROIs) of the image far from the science target and computes the mean pixel values within each region. These values represent the background noise in the image. Finally, the background is estimated as the avarage value across all the regions. \n","    \n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def print_roi_mean_values(image, label=''):\n","    '''\n","    Print mean values of 4 Region Of Interest in the corners \n","    of the image.\n","    Return average value in the ROIs\n","    We use it as background value indicator\n","    '''\n","    bkg_roi1 = image[50:100, 50:100].mean()\n","    bkg_roi2 = image[50:100, -100:-50].mean()\n","    bkg_roi3 = image[-100:-50, 50:100].mean()\n","    bkg_roi4 = image[-100:-50, -100:-50].mean()\n","    bkg = np.mean([bkg_roi1, bkg_roi2, bkg_roi3, bkg_roi4]) \n","    print(\"%s : ROIs mean values %g %g %g %g - Average %g ADU\" % (label, bkg_roi1, bkg_roi2, bkg_roi3, bkg_roi4, bkg) )\n","    return bkg"]},{"cell_type":"markdown","metadata":{},"source":["Once we check that the background across our image is constant, we get its **master** with a function like:\n","- `make_master_image()`: processes a raw data cube by subtracting a background image from each frame of the data cube and then combining the frames into a single master image.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_master_image(raw_data_cube, background_image):\n","    ''' \n","    Compute the master image from a raw cube data and the background image.\n","    \n","    1. Dark image is subtracted to each image of the raw cube\n","    2. Cube is cumulated (without shift and add)\n","    \n","    Dark_image must have the same camera setting\n","    (i.e. exposure and filter) of each image of the raw cube\n","    '''    \n","    \n","    Nframes = raw_data_cube.shape[-1]\n","    \n","    # create the background master \n","    master_background = np.median(np.atleast_3d(background_image), axis = -1)\n","\n","    \n","    # display the background image\n","    plt.figure()\n","    plt.imshow(master_background, vmin=0,vmax=2000)\n","    #plt.imshow(master_background)\n","    plt.colorbar(label = 'ADU')\n","    plt.title(\"Background image\")\n","    print_roi_mean_values(master_background, label='Background')\n"," \n","    # display one frame of the raw data cube\n","    plt.figure()\n","    plt.imshow(raw_data_cube[:,:,0], vmin=0, vmax=2000)\n","    plt.colorbar(label = 'ADU')\n","    plt.title(\"Raw data image #0\")\n","    print_roi_mean_values(raw_data_cube[:,:,0], label='Raw image #0')\n","    \n","    \n","    # subtracting background from raw data\n","    # make sure new array is float !!! to avoid integer overflows\n","    background_subtracted_data_cube = np.zeros(raw_data_cube.shape, dtype=float)\n","    for frame in np.arange(Nframes):\n","        background_subtracted_data_cube[:, :, frame] = raw_data_cube[ :, :, frame] - master_background\n","    \n","\n","    ##########  Computing master image\n","    # Sum along the NDIT dimension to obtain a single (512,640) image\n","    # Must know total integration time\n","    # \n","    # Advanced: shift & add. Detects every image's maximum and shift \n","    # every image before stacking to eliminate residual tip-tilt\n","    ###########    \n","    master_image = background_subtracted_data_cube.sum(axis = -1)\n","    print_roi_mean_values(master_image, label='Master image')\n","\n","    \n","    #########\n","    # Check background subtraction is ok.\n","    # Dark area should be around 0 \n","    ######\n","    plt.figure()\n","    plt.imshow(master_image, vmin=-10, vmax=100)\n","    plt.title('Master image (linear scale, clipped)')\n","    plt.colorbar()\n","\n","    #########\n","    # Display image \n","    # Dark area should be around 0 \n","    ######\n","    plt.figure()\n","    arr=master_image\n","    plt.imshow(np.log10(arr-np.median(arr)+1), cmap='inferno')\n","    plt.title('Master image (log scale)')\n","    plt.colorbar()\n","        \n","    return master_image"]},{"cell_type":"markdown","metadata":{},"source":["So with our defined function, we can proceed with subtracting the background from the open and closed data and get our master images."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["## Reducing closed loop data\n","background_image = InfraredExampleData.get_camera_dark_data()\n","cl_raw_image_cube = InfraredExampleData.get_close_loop_data_cube()   \n","cl_master = make_master_image(cl_raw_image_cube, background_image)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["## Reducing open loop data\n","ol_raw_image_cube = InfraredExampleData.get_open_loop_data_cube()    \n","ol_master = make_master_image(ol_raw_image_cube, background_image)"]},{"cell_type":"markdown","metadata":{},"source":["Now we can display our open loop and closed loop master images.\n","\n","As a visual check, we can zoom into the PSF and remove negative values to display the images on a logarithmic scale."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["ol_ima = ol_master[200:340, 290:430]\n","cl_ima = cl_master[200:340, 290:430]\n","\n","\n","fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(10, 5))  # Adjusting figure size for better layout\n","fig.suptitle('Long Exposure PSF')\n","\n","# Plot the first image and its colorbar (Open Loop)\n","im0 = axs[0].imshow(np.log10(np.clip(ol_ima, 0, None) + 1), cmap='inferno')\n","axs[0].title.set_text('Open Loop')\n","fig.colorbar(im0, ax=axs[0])  # Add colorbar to the first plot\n","\n","# Plot the second image and its colorbar (Close Loop)\n","im1 = axs[1].imshow(np.log10(np.clip(cl_ima, 0, None) + 1), cmap='inferno')\n","axs[1].title.set_text('Close Loop')\n","fig.colorbar(im1, ax=axs[1])  # Add colorbar to the second plot\n","\n","fig.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout so that title doesn't overlap with the subplots\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 Warnings:\n","\n","Well, we went too easy so far :-) ! \n","\n","We need to keep in mind the following key aspects, that are crucial for noise subtraction and to improve the scientific return.\n","\n","Among these, we find:\n","\n","- **Image Subtraction**: when subtracting the background from the raw images they must be taken with the same exposure time.\n","\n","- **Check Camera Saturation**: when integrating the raw images into a master image, itâ€™s important to ensure that the camera has not saturated in the brighter regions of the science target. If the peak of the PSF becomes saturated, the FWHM will be underestimated.\n","\n","- **Background inspection and estimation**: in the previous example, we assumed that the background was constant across the field. However, in real observations the background level can vary spatially due to factors such as uneven sky brightness, and it's important to account for this variation when performing background subtraction.\n","\n","- **Detector artefact**: detectors often contain hot or cold pixels (pixels that exhibit spurius high or low signal levels), which may result in larger readout noise. These artefacts pollutes measurements and should be identified and corrected during image processing.\n","\n","- **Shift & Add**: in image processing, it is used to improve image quality, particularly when combining multiple images into a data cube, by compensating for slight movements in the centroid of the PSF between successive images, which can occur due to atmospheric turbulence (i.e. Tip/Tilt) or telescope's tracking drifts.\n","\n","\n","\n","**NB**: \n","\n","In this tutorial, our primary focus is on reducing background noise in images to evaluate the performance of AO-assisted observations. As mentioned in the introduction, performing scientific analysis on the final master image requires careful calibration using auxiliary images, such as bias, flat, and dark frames. \n","\n","Furthermore, it's important to note that, for scientific analysis, additional steps are required beyond background subtraction. For instance, if we were performig photometriy, we would need to convert the analog-to-digital units (ADU) of the science camera into physical flux units to quantify the brightness of objects. Similarly, if the goal is to perform astrometry, we would need to into account for the pixel scale and correct any distortions in the image to accurately measure objects' position and study their kinematics.\n","\n","For a comprehensive discussion of these topics, see [(J. Heidt, 2022)](https://ui.adsabs.harvard.edu/abs/2022ASSL..467.....H/abstract).\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1.4 Tasks\n","\n","Now it's your turn :-) ! Can you improve the image reduction routine taking into account the above warnings?\n","\n","Particularly, improve the reduction process by:\n","\n","1. Checking if the integrated image is saturated.\n","2. Checking that the background is constant across the field. \n","3. Detect and remove possible detector artefacts (such as hot or cold pixels). \n","4. Apply Shift & Add before integrating the reduced image."]}],"metadata":{"kernelspec":{"display_name":"trash","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":2}
